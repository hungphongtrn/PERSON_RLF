{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backbone_path': 'siglip_checkpoints', 'training': True, 'val_dataset': False, 'temperature': 0.02, 'img_aug': True, 'cmt_depth': 4, 'lr_factor': 5.0, 'MLM': True, 'loss_names': ['sdm', 'id', 'mlm', 'itc', 'cmpm'], 'mlm_loss_weight': 1.0, 'id_loss_weight': 1.0, 'img_size': [384, 128], 'interpolate_pos_encoding': True, 'stride_size': 16, 'text_length': 64, 'vocab_size': 250001, 'optimizer': 'AdamW', 'lr': 1e-05, 'bias_lr_factor': 2.0, 'momentum': 0.9, 'weight_decay': 4e-05, 'weight_decay_bias': 0.0, 'alpha': 0.9, 'beta': 0.999, 'num_epochs': 60, 'milestones': [20, 50], 'gamma': 0.1, 'warmup_factor': 0.1, 'warmup_epochs': 5, 'warmup_method': 'linear', 'lrscheduler': 'cosine', 'target_lr': 0, 'power': 0.9, 'dataset_name': 'VN3K', 'sampler': 'random', 'num_intance': 4, 'root_dir': '.', 'batch_size': 128, 'test_batch_size': 512, 'num_workers': 4}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = OmegaConf.load(\"config.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phongtnh/miniconda3/envs/person_search/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2024-06-10 21:55:07.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatasets.vn3k\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m=> VN3K Images and Captions are loaded\u001b[0m\n",
      "\u001b[32m2024-06-10 21:55:07.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatasets.bases\u001b[0m:\u001b[36mshow_dataset_info\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mVN3K Dataset statistics:\u001b[0m\n",
      "\u001b[32m2024-06-10 21:55:07.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatasets.bases\u001b[0m:\u001b[36mshow_dataset_info\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m\n",
      "+--------+------+--------+----------+\n",
      "| subset | ids  | images | captions |\n",
      "+--------+------+--------+----------+\n",
      "| train  | 2000 |  4302  |  17208   |\n",
      "|  test  | 1000 |  2000  |   8000   |\n",
      "|  val   |  0   |   0    |    0     |\n",
      "+--------+------+--------+----------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VN3K\n"
     ]
    }
   ],
   "source": [
    "from lightning_data import IRRADataModule\n",
    "\n",
    "dm = IRRADataModule(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dm.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 384, 128])\n",
      "Caption shape: torch.Size([1, 34])\n"
     ]
    }
   ],
   "source": [
    "print(\"Image shape:\", train[0]['images'].shape)\n",
    "print(\"Caption shape:\", train[0]['caption_input']['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-10 21:43:39.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlightning_data\u001b[0m:\u001b[36mtrain_dataloader\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1musing random sampler\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-10 21:43:57.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.build\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mTraining Model with ['sdm', 'id', 'mlm', 'itc', 'cmpm'] tasks\u001b[0m\n",
      "\u001b[32m2024-06-10 21:43:59.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodel.build\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mEmbedding Dimension: 768\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "from lighning_models import LitIRRA\n",
    "\n",
    "model = LitIRRA(config, img_loader=test_loader[0], text_loader=test_loader[1], num_classes=dm.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 2 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "trainer = L.Trainer(fast_dev_run=2, precision=\"bf16-mixed\", logger=logger, num_sanity_val_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | IRRA | 595 M \n",
      "-------------------------------\n",
      "595 M     Trainable params\n",
      "0         Non-trainable params\n",
      "595 M     Total params\n",
      "2,382.905 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5.0 times learning rate for random init module \n",
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s, temperature_step=0.020, itc_loss_step=0.000, sdm_loss_step=0.000, cmpm_loss_step=0.000, id_loss_step=7.610, img_acc_step=0.000, txt_acc_step=0.000, mlm_loss_step=12.40, mlm_acc_step=0.000, temperature_epoch=0.020, itc_loss_epoch=0.000, sdm_loss_epoch=0.000, cmpm_loss_epoch=0.000, id_loss_epoch=7.590, img_acc_epoch=0.000, txt_acc_epoch=0.000, mlm_loss_epoch=12.40, mlm_acc_epoch=0.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s, temperature_step=0.020, itc_loss_step=0.000, sdm_loss_step=0.000, cmpm_loss_step=0.000, id_loss_step=7.610, img_acc_step=0.000, txt_acc_step=0.000, mlm_loss_step=12.40, mlm_acc_step=0.000, temperature_epoch=0.020, itc_loss_epoch=0.000, sdm_loss_epoch=0.000, cmpm_loss_epoch=0.000, id_loss_epoch=7.590, img_acc_epoch=0.000, txt_acc_epoch=0.000, mlm_loss_epoch=12.40, mlm_acc_epoch=0.000]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # Initialize dictionary to hold batched data\n",
    "    batched_data = {}\n",
    "\n",
    "    # Get all keys from the first item in the batch to identify field names\n",
    "    keys = batch[0].keys()\n",
    "\n",
    "    for key in keys:\n",
    "        if isinstance(batch[0][key], dict):\n",
    "            # If the field is a nested dictionary, process each sub-field\n",
    "            sub_keys = batch[0][key].keys()\n",
    "            batched_data[key] = {}\n",
    "            for sub_key in sub_keys:\n",
    "                sub_values = [item[key][sub_key] for item in batch]\n",
    "                if isinstance(sub_values[0], torch.Tensor):\n",
    "                    # Concatenate tensors\n",
    "                    batched_data[key][sub_key] = torch.cat(sub_values, dim=0)\n",
    "                else:\n",
    "                    # Assume list of primitive types and convert to tensor\n",
    "                    batched_data[key][sub_key] = torch.tensor(sub_values, dtype=torch.int64)\n",
    "        else:\n",
    "            # If the field is not a nested dictionary, process directly\n",
    "            values = [item[key] for item in batch]\n",
    "            if isinstance(values[0], torch.Tensor):\n",
    "                # Stack tensors if they are of the same size\n",
    "                batched_data[key] = torch.stack(values)\n",
    "            else:\n",
    "                # Assume list of primitive types and convert to tensor\n",
    "                batched_data[key] = torch.tensor(values, dtype=torch.int64)\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "train = dm.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pids': 0,\n",
       " 'image_ids': 0,\n",
       " 'images': tensor([[[ 0.0617,  0.0763,  0.0471,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [ 0.1055,  0.1055,  0.0617,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [ 0.0325,  0.0471,  0.0325,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          ...,\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923]],\n",
       " \n",
       "         [[-0.6415, -0.6565, -0.6865,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-0.5965, -0.6265, -0.6715,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-0.6565, -0.6715, -0.7016,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          ...,\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521]],\n",
       " \n",
       "         [[-0.3853, -0.3711, -0.3995,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-0.3284, -0.3284, -0.3711,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-0.3711, -0.3711, -0.3995,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          ...,\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802]]]),\n",
       " 'caption_input': {'input_ids': tensor([[  1672,    355,   2353,   1735, 250000,    297,    317, 250000,    270,\n",
       "             594,   6701,    326,   1335, 250000,    259,    471,   2567,  78066,\n",
       "            1452,    273,    259,    329, 250000,    677,   1262,    270,    885,\n",
       "          250000,   2342,   1452,    273,    355,   2071,    534,   1079,    594,\n",
       "            1849,    259,    471,    268,    819,   1313,   1452,    273,    259,\n",
       "             329, 250000,    677,   1262,    270,    259,    793,   3767, 250000,\n",
       "            3571,    718,  48066, 250000,    924,  71949,   1452,    273,    355,\n",
       "               1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " 'mlm_inputs': {'input_ids': tensor([[  1672,    355,   2353,   1735, 250000,    297,    317, 250000,    270,\n",
       "             594,   6701,    326,   1335, 250000,    259,    471,   2567,  78066,\n",
       "            1452,    273,    259,    329, 250000,    677,   1262,    270,    885,\n",
       "          250000,   2342,   1452,    273,    355,   2071,    534,   1079,    594,\n",
       "            1849,    259,    471,    268,    819,   1313,   1452,    273,    259,\n",
       "             329, 250000,    677,   1262,    270,    259,    793,   3767, 250000,\n",
       "            3571,    718,  48066, 250000,    924,  71949,   1452,    273,    355,\n",
       "               1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " 'mlm_labels': tensor([[    1,     1,     1,     1, 31029,     1,     1,  2289,     1,     1,\n",
       "              1,     1,     1,   297,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,  4428,     1,     1,     1,     1,   562,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,  4428,     1,     1,     1,\n",
       "              1,     1,     1, 11095,     1,     1, 48066,   394,     1,     1,\n",
       "              1,     1,     1,     1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
