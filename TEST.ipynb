{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phongtnh/miniconda3/envs/person_search/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VN3K\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = OmegaConf.load(\"clip.yaml\")\n",
    "config\n",
    "from lightning_data import IRRADataModule\n",
    "\n",
    "dm = IRRADataModule(config)\n",
    "dm.setup()\n",
    "train = dm.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pids': tensor([0]),\n",
       " 'image_ids': tensor([1]),\n",
       " 'images': tensor([[[-1.7923, -1.7923, -1.7923,  ..., -0.3178, -0.3762, -0.4054],\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -0.2448, -0.2886, -0.2156],\n",
       "          [-1.7923, -1.7923, -1.7923,  ...,  0.3391,  0.3099,  0.3391],\n",
       "          ...,\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923]],\n",
       " \n",
       "         [[-1.7521, -1.7521, -1.7521,  ..., -0.2213, -0.2963, -0.3264],\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -0.1463, -0.1913, -0.1313],\n",
       "          [-1.7521, -1.7521, -1.7521,  ...,  0.4240,  0.3940,  0.4390],\n",
       "          ...,\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521]],\n",
       " \n",
       "         [[-1.4802, -1.4802, -1.4802,  ..., -0.1009, -0.1578, -0.1578],\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -0.0298, -0.0724,  0.0271],\n",
       "          [-1.4802, -1.4802, -1.4802,  ...,  0.5532,  0.5248,  0.5675],\n",
       "          ...,\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802]]]),\n",
       " 'mlm_label': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,   267,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 19820,\n",
       "           339,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   705,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100]),\n",
       " 'caption_input_ids': tensor([49406,   320,  4852,   268, 10941,  1888,   786,   267,  3309,   736,\n",
       "           268, 36800,  1449,  4079,   267,   320,  3005,   268,  1709, 19820,\n",
       "           339,   268,  2523,   537,  1395,  7048, 23172,   267, 11982,   705,\n",
       "           787, 23172,   269, 49407,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100]),\n",
       " 'mlm_input_ids': tensor([49406,   320,  4852,   268, 10941,  1888,   786, 49408,  3309,   736,\n",
       "           268, 36800,  1449,  4079,   267,   320,  3005,   268,  1709, 49408,\n",
       "         49408,   268,  2523,   537,  1395,  7048, 23172,   267, 11982, 49408,\n",
       "           787, 23172,   269, 49407,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dm.train_dataloader()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_ids': tensor([[3803],\n",
       "         [3803],\n",
       "         [3803],\n",
       "         [3803]]),\n",
       " 'images': tensor([[[[-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           ...,\n",
       "           [-0.0113, -0.0113,  0.0033,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-0.0696, -0.0842, -0.0988,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-0.1134, -0.1426, -0.1572,  ..., -1.7923, -1.7923, -1.7923]],\n",
       " \n",
       "          [[-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           ...,\n",
       "           [-0.0262, -0.0262, -0.0112,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-0.0862, -0.1012, -0.1163,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-0.1313, -0.1613, -0.1763,  ..., -1.7521, -1.7521, -1.7521]],\n",
       " \n",
       "          [[-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           ...,\n",
       "           [ 0.0129,  0.0129,  0.0271,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-0.0440, -0.0582, -0.0724,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-0.0867, -0.1151, -0.1293,  ..., -1.4802, -1.4802, -1.4802]]],\n",
       " \n",
       " \n",
       "         [[[-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           ...,\n",
       "           [ 0.1931,  0.1931,  0.2077,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [ 0.1785,  0.1785,  0.1785,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [ 0.1785,  0.1639,  0.1639,  ..., -1.7923, -1.7923, -1.7923]],\n",
       " \n",
       "          [[-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           ...,\n",
       "           [ 0.1389,  0.1539,  0.1689,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [ 0.1239,  0.1389,  0.1389,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [ 0.1389,  0.1239,  0.1239,  ..., -1.7521, -1.7521, -1.7521]],\n",
       " \n",
       "          [[-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           ...,\n",
       "           [ 0.2262,  0.2262,  0.2404,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [ 0.2120,  0.2120,  0.2120,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [ 0.2120,  0.1977,  0.1977,  ..., -1.4802, -1.4802, -1.4802]]],\n",
       " \n",
       " \n",
       "         [[[-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           ...,\n",
       "           [-0.0696, -0.0842, -0.0988,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-0.1134, -0.1426, -0.1572,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-0.1572, -0.1718, -0.1426,  ..., -1.7923, -1.7923, -1.7923]],\n",
       " \n",
       "          [[-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           ...,\n",
       "           [-0.0862, -0.1012, -0.1163,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-0.1313, -0.1613, -0.1763,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-0.1763, -0.1913, -0.1763,  ..., -1.7521, -1.7521, -1.7521]],\n",
       " \n",
       "          [[-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           ...,\n",
       "           [-0.0440, -0.0582, -0.0724,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-0.0867, -0.1151, -0.1293,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-0.1151, -0.1435, -0.1293,  ..., -1.4802, -1.4802, -1.4802]]],\n",
       " \n",
       " \n",
       "         [[[-1.7923, -1.7923, -1.7923,  ..., -0.9164, -0.9164, -0.8434],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -0.9310, -0.8872, -0.7996],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -0.9310, -0.8726, -0.8288],\n",
       "           ...,\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923]],\n",
       " \n",
       "          [[-1.7521, -1.7521, -1.7521,  ..., -0.9567, -0.9417, -0.8366],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -0.9717, -0.8967, -0.7916],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -0.9567, -0.8516, -0.8066],\n",
       "           ...,\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "           [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521]],\n",
       " \n",
       "          [[-1.4802, -1.4802, -1.4802,  ..., -0.8403, -0.8261, -0.7408],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -0.8545, -0.7834, -0.6839],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -0.8545, -0.7550, -0.6839],\n",
       "           ...,\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802]]]]),\n",
       " 'mlm_label': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   318,  1303,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          36842,  -100,  -100,    83,  -100,  -100,  -100,  -100,  -100,    76,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,   157,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,   340,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   635,   846,  -100,  -100,  -100,  -100,  -100,  -100,   328,\n",
       "             76,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  2556,  -100,  1449,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  7067,  2349,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  1449,  -100,  -100,\n",
       "           -100,  -100,   269,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "         [ -100,   320,  -100,  2556,   593,  -100,  -100, 10130,  -100,  -100,\n",
       "           -100,  -100,   320,  -100,  -100,   320,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  3309,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   269,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,   318,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,   115,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,   118,   107,  -100,    77,  -100,\n",
       "           -100,  -100,  -100,    67,  -100,  -100,  -100]]),\n",
       " 'mlm_input_ids': tensor([[49406,    76,   157,   119,   247,   339,    71,   157,   119,   235,\n",
       "            322, 49408, 49408,   327,    77,   157,   119,   363,    66, 27733,\n",
       "          49408,   328,   318, 49408,  8891,   322,    77,  5674, 36802, 49408,\n",
       "          36149,   340,   128,   239,   576,   267,    76, 49408,   118,   115,\n",
       "            322,  7261,   334,    77,   157,   119,   487,    66, 27733,    76,\n",
       "            129,   358,    76, 36149, 49408,    77, 26170,   340,    65, 27515,\n",
       "            333, 49408, 49408,   267,    65, 27515,   333, 38740, 36149, 49408,\n",
       "          49408,   157,   118,   115,   322,  7261,   334],\n",
       "         [49406,   320,  3970, 49408,   593, 49408,  2225,   267,  3309,   550,\n",
       "          14188,  6164,   593,   320,  1579, 49408, 49408,   537,  1449,  4909,\n",
       "           2349,   267,  3309,  1449,  5003,   267,  3309,  1449, 17397,   593,\n",
       "           1579, 22682, 44456, 49407,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 49408, 49408,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100, 49408,  -100,  -100,\n",
       "           -100,  -100, 49408,  -100,  -100,  -100,  -100, 49408,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "         [49406, 49408,  3970, 49408, 49408,  8476,   268, 49408,  1449,  2225,\n",
       "            267,  3309, 49408, 22442,   593, 49408,  2866,  3801,  2912,   267,\n",
       "           2782,  3309,   320,  6164,   593,   320,  1579,  7067,  2349,   537,\n",
       "           1449,  4909,  2349,   267, 49408,  1449,  5003,   267,  3309,  1449,\n",
       "           4079, 49408, 49407,  -100, 49408,  -100,  -100,  -100, 49408,  -100,\n",
       "           -100,  -100,  -100,  -100, 49408,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100, 49408,  -100,  -100,  -100,  -100,  -100],\n",
       "         [49406,    76,   157,   119,   247,   339,    71,   157,   119,   235,\n",
       "            322,   318,  1303,   327,    77,   157,   119,   363,    66, 27733,\n",
       "          36842,   328, 49408,    83,  8891,   322,    76, 36149,   340,   128,\n",
       "            239,   576,   267,    76,   157,   118, 49408,   322,  7261,   334,\n",
       "            318, 14022,  7261,   322,    65, 27515,   333, 38740, 36149,   328,\n",
       "             66, 27733,    77,   157,   119,   255,   320,   635, 27515,   333,\n",
       "             76, 36149,   340,   635,   157, 49408, 49408,  5215,    77,   157,\n",
       "            119,   255,   320, 49408,   130,   108,   157]]),\n",
       " 'caption_input_ids': tensor([[49406,    76,   157,   119,   247,   339,    71,   157,   119,   235,\n",
       "            322,   318,  1303,   327,    77,   157,   119,   363,    66, 27733,\n",
       "          36842,   328,   318,    83,  8891,   322,    77,  5674, 36802,    76,\n",
       "          36149,   340,   128,   239,   576,   267,    76,   157,   118,   115,\n",
       "            322,  7261,   334,    77,   157,   119,   487,    66, 27733,    76,\n",
       "            129,   358,    76, 36149,   340,    77, 26170,   340,    65, 27515,\n",
       "            333,   635,   846,   267,    65, 27515,   333, 38740, 36149,   328,\n",
       "             76,   157,   118,   115,   322,  7261,   334],\n",
       "         [49406,   320,  3970,  2556,   593,  1449,  2225,   267,  3309,   550,\n",
       "          14188,  6164,   593,   320,  1579,  7067,  2349,   537,  1449,  4909,\n",
       "           2349,   267,  3309,  1449,  5003,   267,  3309,  1449, 17397,   593,\n",
       "           1579, 22682,   269, 49407,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "         [49406,   320,  3970,  2556,   593,  8476,   268, 10130,  1449,  2225,\n",
       "            267,  3309,   320, 22442,   593,   320,  2866,  3801,  2912,   267,\n",
       "           2782,  3309,   320,  6164,   593,   320,  1579,  7067,  2349,   537,\n",
       "           1449,  4909,  2349,   267,  3309,  1449,  5003,   267,  3309,  1449,\n",
       "           4079,   269, 49407,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "         [49406,    76,   157,   119,   247,   339,    71,   157,   119,   235,\n",
       "            322,   318,  1303,   327,    77,   157,   119,   363,    66, 27733,\n",
       "          36842,   328,   318,    83,  8891,   322,    76, 36149,   340,   128,\n",
       "            239,   576,   267,    76,   157,   118,   115,   322,  7261,   334,\n",
       "            318, 14022,  7261,   322,    65, 27515,   333, 38740, 36149,   328,\n",
       "             66, 27733,    77,   157,   119,   255,   320,   635, 27515,   333,\n",
       "             76, 36149,   340,   635,   157,   118,   107,  5215,    77,   157,\n",
       "            119,   255,   320,    67,   130,   108,   157]]),\n",
       " 'pids': tensor([[1750],\n",
       "         [1750],\n",
       "         [1750],\n",
       "         [1750]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from utils.tokenizer_utils import get_tokenizer\n",
    "\n",
    "config = OmegaConf.load(\"clip.yaml\")\n",
    "config\n",
    "\n",
    "tokenizer = get_tokenizer(config.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipTokenizer(name_or_path='/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints', vocab_size=250100, model_max_length=64, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t1: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t250000: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_mask = [\n",
    "    [1 if val in tokenizer.special_token_ids else 0 for val in label]\n",
    "    for label in labels.tolist()\n",
    "]\n",
    "special_tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_matrix.masked_fill_(\n",
    "    torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True,  True, False, False, False,  True,  True,\n",
       "         False,  True, False, False, False, False, False, False, False,  True,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[~masked_indices] = -100  # We only compute loss on masked tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100, 25972,  -100,   259,   262,  -100,  -100,  -100,   347,   259,\n",
       "          -100, 33264,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   287,\n",
       "         79995,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         33264,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "labels[~masked_indices] = -100  # We only compute loss on masked tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,   259,  -100,  -100,  -100,  -100, 94221,  -100,   287,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m indices_replaced \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbernoulli(torch\u001b[38;5;241m.\u001b[39mfull(labels\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m0.8\u001b[39m))\u001b[38;5;241m.\u001b[39mbool() \u001b[38;5;241m&\u001b[39m masked_indices\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m inputs[indices_replaced] \u001b[38;5;241m=\u001b[39m mask_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask_token' is not defined"
     ]
    }
   ],
   "source": [
    "indices_replaced = (\n",
    "    torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    ")\n",
    "inputs[indices_replaced] = mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_check = [\"caption_input\", \"mlm_inputs\", \"mlm_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 384, 128])\n",
      "Caption shape: torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Image shape:\", train[0][\"images\"].shape)\n",
    "print(\"Caption shape:\", train[0][\"caption_input\"][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total training steps\n",
    "num_training_steps = len(train_loader) * config.max_epochs\n",
    "num_warmup_steps = int(num_training_steps * config.warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2151"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lighning_models import LitIRRA\n",
    "\n",
    "model = LitIRRA(\n",
    "    config,\n",
    "    img_loader=test_loader[0],\n",
    "    text_loader=test_loader[1],\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_classes=dm.num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': tensor(0.0200),\n",
       " 'sdm_loss': tensor(0., grad_fn=<AddBackward0>),\n",
       " 'id_loss': tensor(7.5937, grad_fn=<MulBackward0>),\n",
       " 'img_acc': tensor(0.),\n",
       " 'txt_acc': tensor(0.),\n",
       " 'mlm_loss': tensor(12.3301, grad_fn=<MulBackward0>),\n",
       " 'mlm_acc': tensor(0.)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = next(iter(train_loader))[\"images\"]\n",
    "test_caption = next(iter(train_loader))[\"caption_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encode_image(test_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encode_text(test_caption).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"eval_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "trainer = L.Trainer(\n",
    "    precision=\"bf16-mixed\",\n",
    "    logger=logger,\n",
    "    max_epochs=config.num_epochs,\n",
    "    accumulate_grad_batches=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer.predict` requires `forward` method to run.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpredict(model, test_loader)\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:864\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    866\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:903\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    900\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    902\u001b[0m )\n\u001b[0;32m--> 903\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:937\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m--> 937\u001b[0m _verify_loop_configurations(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    942\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: setting up strategy environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:44\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     42\u001b[0m     __verify_eval_loop_configuration(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mPREDICTING:\n\u001b[0;32m---> 44\u001b[0m     __verify_eval_loop_configuration(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m __verify_batch_transfer_support(trainer)\n\u001b[1;32m     48\u001b[0m __verify_configure_model_configuration(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:105\u001b[0m, in \u001b[0;36m__verify_eval_loop_configuration\u001b[0;34m(model, stage)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`predict_step` cannot be None to run `Trainer.predict`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_step \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, model):\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.predict` requires `forward` method to run.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# verify minimum evaluation requirements\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_step:\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer.predict` requires `forward` method to run."
     ]
    }
   ],
   "source": [
    "trainer.predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # Initialize dictionary to hold batched data\n",
    "    batched_data = {}\n",
    "\n",
    "    # Get all keys from the first item in the batch to identify field names\n",
    "    keys = batch[0].keys()\n",
    "\n",
    "    for key in keys:\n",
    "        if isinstance(batch[0][key], dict):\n",
    "            # If the field is a nested dictionary, process each sub-field\n",
    "            sub_keys = batch[0][key].keys()\n",
    "            batched_data[key] = {}\n",
    "            for sub_key in sub_keys:\n",
    "                sub_values = [item[key][sub_key] for item in batch]\n",
    "                if isinstance(sub_values[0], torch.Tensor):\n",
    "                    # Concatenate tensors\n",
    "                    batched_data[key][sub_key] = torch.cat(sub_values, dim=0)\n",
    "                else:\n",
    "                    # Assume list of primitive types and convert to tensor\n",
    "                    batched_data[key][sub_key] = torch.tensor(\n",
    "                        sub_values, dtype=torch.int64\n",
    "                    )\n",
    "        else:\n",
    "            # If the field is not a nested dictionary, process directly\n",
    "            values = [item[key] for item in batch]\n",
    "            if isinstance(values[0], torch.Tensor):\n",
    "                # Stack tensors if they are of the same size\n",
    "                batched_data[key] = torch.stack(values)\n",
    "            else:\n",
    "                # Assume list of primitive types and convert to tensor\n",
    "                batched_data[key] = torch.tensor(values, dtype=torch.int64)\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "train = dm.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pids': 0,\n",
       " 'image_ids': 0,\n",
       " 'images': tensor([[[ 0.0617,  0.0763,  0.0471,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [ 0.1055,  0.1055,  0.0617,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [ 0.0325,  0.0471,  0.0325,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          ...,\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
       "          [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923]],\n",
       " \n",
       "         [[-0.6415, -0.6565, -0.6865,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-0.5965, -0.6265, -0.6715,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-0.6565, -0.6715, -0.7016,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          ...,\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
       "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521]],\n",
       " \n",
       "         [[-0.3853, -0.3711, -0.3995,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-0.3284, -0.3284, -0.3711,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-0.3711, -0.3711, -0.3995,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          ...,\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
       "          [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802]]]),\n",
       " 'caption_input': {'input_ids': tensor([[  1672,    355,   2353,   1735, 250000,    297,    317, 250000,    270,\n",
       "             594,   6701,    326,   1335, 250000,    259,    471,   2567,  78066,\n",
       "            1452,    273,    259,    329, 250000,    677,   1262,    270,    885,\n",
       "          250000,   2342,   1452,    273,    355,   2071,    534,   1079,    594,\n",
       "            1849,    259,    471,    268,    819,   1313,   1452,    273,    259,\n",
       "             329, 250000,    677,   1262,    270,    259,    793,   3767, 250000,\n",
       "            3571,    718,  48066, 250000,    924,  71949,   1452,    273,    355,\n",
       "               1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " 'mlm_inputs': {'input_ids': tensor([[  1672,    355,   2353,   1735, 250000,    297,    317, 250000,    270,\n",
       "             594,   6701,    326,   1335, 250000,    259,    471,   2567,  78066,\n",
       "            1452,    273,    259,    329, 250000,    677,   1262,    270,    885,\n",
       "          250000,   2342,   1452,    273,    355,   2071,    534,   1079,    594,\n",
       "            1849,    259,    471,    268,    819,   1313,   1452,    273,    259,\n",
       "             329, 250000,    677,   1262,    270,    259,    793,   3767, 250000,\n",
       "            3571,    718,  48066, 250000,    924,  71949,   1452,    273,    355,\n",
       "               1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " 'mlm_labels': tensor([[    1,     1,     1,     1, 31029,     1,     1,  2289,     1,     1,\n",
       "              1,     1,     1,   297,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,  4428,     1,     1,     1,     1,   562,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,  4428,     1,     1,     1,\n",
       "              1,     1,     1, 11095,     1,     1, 48066,   394,     1,     1,\n",
       "              1,     1,     1,     1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_config = {\n",
    "    \"architectures\": [\"SiglipModel\"],\n",
    "    \"initializer_factor\": 1.0,\n",
    "    \"model_type\": \"siglip\",\n",
    "    \"text_config\": {\n",
    "        \"model_type\": \"siglip_text_model\",\n",
    "        \"vocab_size\": 250001,\n",
    "        \"max_position_embeddings\": 77,\n",
    "    },\n",
    "    \"torch_dtype\": \"float32\",\n",
    "    \"transformers_version\": \"4.37.0.dev0\",\n",
    "    \"vision_config\": {\"image_size\": [384, 128], \"model_type\": \"siglip_vision_model\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model.siglip.modeling_siglip.SiglipModel() argument after ** must be a mapping, not SiglipConfig",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msiglip\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_siglip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SiglipConfig\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m SiglipConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msiglip_config)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m SiglipModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "\u001b[0;31mTypeError\u001b[0m: model.siglip.modeling_siglip.SiglipModel() argument after ** must be a mapping, not SiglipConfig"
     ]
    }
   ],
   "source": [
    "from model.siglip.modeling_siglip import SiglipModel\n",
    "from model.siglip.configuration_siglip import SiglipConfig\n",
    "\n",
    "config = SiglipConfig(**siglip_config)\n",
    "model = SiglipModel(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # Check the name of position embeddings of the text model\n",
    "    if \"text\" in name and \"position\" in name and \"embed\" in name:\n",
    "        text_model_position_embedding = name\n",
    "        text_model_position_embedding_shape = param.shape[0]\n",
    "    # Check the name of token embeddings of the text model\n",
    "    if \"text\" in name and \"token\" in name and \"embed\" in name:\n",
    "        text_model_token_embedding = name\n",
    "        text_model_token_embedding_shape = param.shape[0]\n",
    "    # Check the name of position embeddings of the image model\n",
    "    if \"position\" in name and \"embed\" in name:\n",
    "        if \"vision\" in name or \"visual\" in name or \"image\" in name:\n",
    "            image_model_position_embedding = name\n",
    "            image_model_position_embedding_shape = param.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('text_model.embeddings.position_embedding.weight', 77)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model_position_embedding, text_model_position_embedding_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('text_model.embeddings.token_embedding.weight', 250001)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model_token_embedding, text_model_token_embedding_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vision_model.embeddings.position_embedding.weight', 192)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model_position_embedding, image_model_position_embedding_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "state_dict = load_file(\n",
    "    \"/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints/model.safetensors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[text_model_position_embedding].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_bias', 'logit_scale', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_model.head.bias', 'text_model.head.weight', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.head.attention.in_proj_bias', 'vision_model.head.attention.in_proj_weight', 'vision_model.head.attention.out_proj.bias', 'vision_model.head.attention.out_proj.weight', 'vision_model.head.layernorm.bias', 'vision_model.head.layernorm.weight', 'vision_model.head.mlp.fc1.bias', 'vision_model.head.mlp.fc1.weight', 'vision_model.head.mlp.fc2.bias', 'vision_model.head.mlp.fc2.weight', 'vision_model.head.probe', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layer_resize import resize_text_pos_embedding, resize_token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing text model position embedding from 64 to 77\n",
      "torch.Size([77, 768])\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    state_dict[text_model_position_embedding].shape[0]\n",
    "    != text_model_position_embedding_shape\n",
    "):\n",
    "    print(\n",
    "        f\"Resizing text model position embedding from {state_dict[text_model_position_embedding].shape[0]} to {text_model_position_embedding_shape}\"\n",
    "    )\n",
    "    state_dict[text_model_position_embedding] = resize_text_pos_embedding(\n",
    "        state_dict[text_model_position_embedding],\n",
    "        target_dim=text_model_position_embedding_shape,\n",
    "    )\n",
    "    print(state_dict[text_model_position_embedding].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[text_model_position_embedding].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def modify_params_weight(model):\n",
    "    for name, param in text_model.named_parameters():\n",
    "        if \"position\" in name and \"embedding\" in name:\n",
    "            print(name)\n",
    "            position_embedding = resize_text_pos_embedding(param, ignore=\"first\")\n",
    "            print(position_embedding)\n",
    "            param.data = position_embedding\n",
    "            # Update model info\n",
    "            model.config.text_config.position_embedding_size = position_embedding.size(\n",
    "                1\n",
    "            )\n",
    "        # elif 'token' in name and 'embedding' in name:\n",
    "        #     token_embedding = resize_token_embedding(param, ignore=\"first\")\n",
    "        #     param = token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipTextConfig {\n",
       "  \"_name_or_path\": \"siglip_checkpoints\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 49406,\n",
       "  \"eos_token_id\": 49407,\n",
       "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"max_position_embeddings\": 64,\n",
       "  \"model_type\": \"siglip_text_model\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"transformers_version\": \"4.40.2\",\n",
       "  \"vocab_size\": 250000\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.text_model.embeddings.position_embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_model.embeddings.position_embedding.weight\n",
      "Parameter containing:\n",
      "tensor([[-6.4781e-02, -1.3846e-02,  2.2669e-04,  ..., -3.6059e-02,\n",
      "          6.7947e-02,  3.2825e-02],\n",
      "        [-5.6405e-02,  2.2390e-02, -3.7283e-02,  ...,  7.4557e-02,\n",
      "         -3.4371e-03,  6.5015e-02],\n",
      "        [-6.8742e-02,  6.4525e-03,  6.7116e-03,  ...,  2.3484e-02,\n",
      "          2.4183e-02,  5.6726e-02],\n",
      "        ...,\n",
      "        [ 9.5946e-02, -1.4957e-01,  7.9668e-02,  ...,  1.0009e-01,\n",
      "         -2.4498e-02, -1.5330e-02],\n",
      "        [ 1.0203e-02, -3.4751e-02,  1.3561e-01,  ...,  2.2974e-01,\n",
      "         -3.6014e-03,  9.4231e-02],\n",
      "        [-1.2788e-01, -5.5406e-02,  8.7895e-02,  ...,  6.7243e-02,\n",
      "          1.1804e-01,  1.5169e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "modify_params_weight(text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipTextModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(250000, 768)\n",
       "      (position_embedding): Embedding(64, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding = resize_text_pos_embedding(position_embedding, ignore=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in text_model.named_parameters():\n",
    "    if \"position\" in name and \"embedding\" in name:\n",
    "        position_embedding = resize_text_pos_embedding(param, ignore=\"first\")\n",
    "        param = position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipTextModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(250000, 768)\n",
       "      (position_embedding): Embedding(64, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_model_weight(name, weight):\n",
    "    for name, param in text_model.named_parameters():\n",
    "        if name == name:\n",
    "            param.data = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-6.4781e-02, -1.3846e-02,  2.2669e-04,  ..., -3.6059e-02,\n",
       "          6.7947e-02,  3.2825e-02],\n",
       "        [-5.6405e-02,  2.2390e-02, -3.7283e-02,  ...,  7.4557e-02,\n",
       "         -3.4371e-03,  6.5015e-02],\n",
       "        [-7.3000e-02,  9.5201e-04,  2.1896e-02,  ...,  5.8575e-03,\n",
       "          3.3716e-02,  5.3865e-02],\n",
       "        ...,\n",
       "        [ 1.2440e-01, -2.4068e-01,  2.5565e-02,  ..., -3.8673e-02,\n",
       "         -8.7440e-03, -8.2370e-02],\n",
       "        [ 5.7860e-02, -2.7623e-02,  1.5208e-01,  ...,  2.8583e-01,\n",
       "         -4.5584e-02,  7.4401e-02],\n",
       "        [-1.2788e-01, -5.5406e-02,  8.7895e-02,  ...,  6.7243e-02,\n",
       "          1.1804e-01,  1.5169e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipTextModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(250000, 768)\n",
       "      (position_embedding): Embedding(64, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "property 'vocab_size' of 'SiglipTokenizer' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250001\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'vocab_size' of 'SiglipTokenizer' object has no setter"
     ]
    }
   ],
   "source": [
    "tokenizer.vocab_size = 250001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token = tokenizer.convert_ids_to_tokens(250000)\n",
    "tokenizer.add_special_tokens({\"mask_token\": mask_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(lsof -t -i:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5.0 times learning rate for random init module \n"
     ]
    }
   ],
   "source": [
    "_, scheduler = model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x7f31600aead0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phongtnh/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lrs = []\n",
    "for step in range(num_training_steps):\n",
    "    scheduler[0].step()\n",
    "    lrs.append(scheduler[0].get_last_lr()[0])\n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parse_module_str import parse_module_str\n",
    "\n",
    "tokenizer = parse_module_str(\"model.clip.simple_tokenizer.SimpleTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parse_module_str import parse_module_str\n",
    "\n",
    "config = parse_module_str(\"dict\", a=1, b=2, c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.clip.clip import load, available_models\n",
    "\n",
    "available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = load(\"ViT-B/16\", download_root=\"./clip_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\n",
    "    \"/home/phongtnh/Person-Search/person_rlf/VN3K/imgs/DANgoc_01/02885_2.jpg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer_utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\n",
    "    {\"type\": \"model.clip.simple_tokenizer.SimpleTokenizer\", \"vocab_size\": 49409}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mspecial_token_ids\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.special_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49406\n",
      "49407\n",
      "49408\n"
     ]
    }
   ],
   "source": [
    "for token in tokenizer.cache.values():\n",
    "    print(tokenizer.encoder[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from utils.tokenizer_utils import get_tokenizer\n",
    "\n",
    "config = OmegaConf.load(\"siglip.yaml\")\n",
    "config\n",
    "\n",
    "tokenizer = get_tokenizer(config.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'context_length': 64} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  259,   272,  2342, 31029,   297,   259,   272,  9816,   259,   471,\n",
       "           268,  1452,   273,   355,   278,   885,   331,  3633,   562,  2342,\n",
       "           534,  7281,  7020,   594,  1849,   819,  1313, 11095, 28132,  1452,\n",
       "           273,   355,   278,  3767,   718, 48066,   394,   924, 71949,   534,\n",
       "          7281,   259,   270,  1075,  4195,   317,  1544,  1452,   273,   534,\n",
       "          7281,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    text,\n",
    "    context_length=64,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 250000]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get special toke map id\n",
    "tokenizer.special_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cuhkpedes import CUHKPEDES\n",
    "\n",
    "dataset = CUHKPEDES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   589,   533,   320,  1070,  1538,  1538,  1538,  1538,  1538,\n",
       "           1538,  1538,  1538,  1538,  1538,  1538,  1538,  1538,  1538,  1538,\n",
       "           1538,  1538,  1538,  1538,  1538,  1538,  1538,  1538,  1538,  7965,\n",
       "            954, 49407,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    \"This is a very long long long long long long long long long long long long long long long long long long long long long long long long setence\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"This is a very long long long long long long long long long long long long long long long long long long long long long long long long setence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a very long long long long long long long long long long long long long long long long long long long long long long long long setence</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(caption: str, tokenizer, context_length, truncate=True):\n",
    "    inputs = tokenizer(\n",
    "        caption,\n",
    "        context_length=context_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=truncate,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49406)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(test, tokenizer, 77, truncate=True)[\"input_ids\"].squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftext|>this is a very long long long long long long long long long long long long long long long long long long long long long long long long setence <|endoftext|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenize(test, tokenizer, 77, truncate=True)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phongtnh/miniconda3/envs/person_search/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VN3K\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = OmegaConf.load(\"clip.yaml\")\n",
    "config\n",
    "from lightning_data import IRRADataModule\n",
    "\n",
    "dm = IRRADataModule(config)\n",
    "dm.setup()\n",
    "train = dm.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 384, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 77])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['caption_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 77])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['mlm_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.build import build_backbone_with_proper_layer_resize\n",
    "\n",
    "model = build_backbone_with_proper_layer_resize(config.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49409, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 384, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    output = model.get_image_features(\n",
    "        pixel_values=img.to(\"cuda\"),\n",
    "        output_last_hidden_state=True,\n",
    "        return_dict=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 193, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 77])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"caption_input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [52,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_text_features(\n\u001b[1;32m      3\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mtest[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# attention_mask=train[0][\"caption_attention_mask\"].unsqueeze(0).to(\"cuda\"),\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     )\n",
      "File \u001b[0;32m~/Person-Search/person_rlf/model/clip/model.py:406\u001b[0m, in \u001b[0;36mCLIP.get_text_features\u001b[0;34m(self, input_ids, output_last_hidden_state, return_dict)\u001b[0m\n\u001b[1;32m    404\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    405\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[1;32m    407\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    408\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Person-Search/person_rlf/model/clip/model.py:242\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Person-Search/person_rlf/model/clip/model.py:225\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 225\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[1;32m    226\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Person-Search/person_rlf/model/clip/model.py:218\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattention\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x, x, x, need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.get_text_features(\n",
    "        input_ids=test[\"caption_input_ids\"].to(\"cuda\"),\n",
    "        # attention_mask=train[0][\"caption_attention_mask\"].unsqueeze(0).to(\"cuda\"),\n",
    "        return_dict=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def resize_pos_embed(posemb, height, width):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb = posemb.unsqueeze(0)\n",
    "\n",
    "    posemb_token, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(height, width), mode=\"bilinear\")\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, height * width, -1)\n",
    "    posemb = torch.cat([posemb_token, posemb_grid], dim=1)\n",
    "    return posemb.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "state_dict = load_file(\n",
    "    \"/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints/model.safetensors\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posemb = state_dict['vision_model.embeddings.position_embedding.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posemb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0606,  0.0668,  0.0189,  ..., -0.0221, -0.0020, -0.0254],\n",
      "        [ 0.0606,  0.0668,  0.0189,  ..., -0.0221, -0.0020, -0.0254],\n",
      "        [ 0.0606,  0.0668,  0.0189,  ..., -0.0221, -0.0020, -0.0254],\n",
      "        ...,\n",
      "        [ 0.0606,  0.0668,  0.0189,  ..., -0.0221, -0.0020, -0.0254],\n",
      "        [ 0.0606,  0.0668,  0.0189,  ..., -0.0221, -0.0020, -0.0254],\n",
      "        [ 0.0606,  0.0668,  0.0189,  ..., -0.0221, -0.0020, -0.0254]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "include_cls_token = True\n",
    "posemb = posemb.unsqueeze(0)\n",
    "posemb_token = None\n",
    "\n",
    "if include_cls_token:\n",
    "    posemb_grid = posemb[0, :]\n",
    "else:\n",
    "    posemb_token, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "\n",
    "# Reshape into a 2D grid to keep the structure of an image\n",
    "gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "# Interpolate grid of position embeddings\n",
    "posemb_grid = F.interpolate(posemb_grid, size=(24, 8), mode=\"bilinear\")\n",
    "posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, 24 * 8, -1)\n",
    "\n",
    "# Concatenate the first token if it was ignored if the originam posemb had a CLS token\n",
    "if posemb_token:\n",
    "    print(torch.cat([posemb_token, posemb_grid.squeeze(0)], dim=0))\n",
    "else:\n",
    "    print(posemb_grid.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "posemb_token, posemb_grid = posemb[:, :1], posemb[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posemb_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posemb_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "posemb_grid = posemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posemb_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "gs_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posemb_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.0598e-02,  6.6767e-02,  1.8868e-02,  ...,  1.7853e-02,\n",
       "            4.2891e-02, -3.2202e-02],\n",
       "          [ 6.1810e-02,  1.3046e-01,  2.9045e-02,  ..., -2.1822e-02,\n",
       "            4.8408e-02, -3.0584e-02],\n",
       "          [ 5.2671e-02,  9.3742e-02,  1.7111e-02,  ..., -1.7024e-03,\n",
       "            1.5958e-02, -2.9414e-02],\n",
       "          ...,\n",
       "          [ 6.3926e-02,  9.3750e-02,  1.9014e-02,  ..., -1.0340e-02,\n",
       "            3.9934e-02, -3.7885e-02],\n",
       "          [ 6.9173e-02,  4.7911e-02,  2.9768e-02,  ...,  1.4170e-02,\n",
       "           -1.2070e-02, -2.6054e-02],\n",
       "          [ 6.9872e-02,  2.2283e-02,  2.4796e-02,  ...,  7.7103e-03,\n",
       "            7.8076e-03, -2.4767e-02]],\n",
       "\n",
       "         [[ 5.5481e-02,  3.3013e-02,  1.0880e-02,  ...,  2.3799e-02,\n",
       "            2.7303e-02, -3.0774e-02],\n",
       "          [ 5.0566e-02,  5.8090e-02,  1.7412e-02,  ...,  1.5086e-02,\n",
       "            5.7756e-03, -3.0999e-02],\n",
       "          [ 5.4609e-02,  5.2114e-02,  1.7207e-02,  ...,  3.2391e-03,\n",
       "            1.8216e-02, -3.1216e-02],\n",
       "          ...,\n",
       "          [ 6.3781e-02,  6.2438e-02,  1.8038e-02,  ..., -2.9240e-03,\n",
       "            2.9874e-02, -3.5496e-02],\n",
       "          [ 6.3825e-02,  3.1459e-02,  2.0109e-02,  ...,  2.0697e-02,\n",
       "           -2.8478e-02, -2.7277e-02],\n",
       "          [ 6.8524e-02,  3.1524e-02,  1.7233e-02,  ...,  3.6594e-02,\n",
       "           -1.2042e-02, -2.3818e-02]],\n",
       "\n",
       "         [[ 5.7720e-01,  6.1422e-01,  4.5688e-01,  ...,  1.0938e-01,\n",
       "           -9.4859e-02, -1.7600e-01],\n",
       "          [ 5.4651e-02,  1.7004e-02,  2.5524e-02,  ...,  2.8953e-02,\n",
       "            6.3525e-02, -3.1373e-02],\n",
       "          [ 7.4117e-02, -1.0091e-01,  1.4648e-02,  ...,  4.2114e-03,\n",
       "            5.4334e-02, -3.4289e-02],\n",
       "          ...,\n",
       "          [ 7.9859e-02, -4.1403e-02,  1.7698e-02,  ...,  2.3389e-02,\n",
       "            4.0259e-02, -2.3389e-02],\n",
       "          [ 5.8679e-02,  2.9906e-02,  2.3791e-02,  ...,  3.0677e-02,\n",
       "            2.2478e-02, -3.1361e-02],\n",
       "          [-1.4604e-01,  3.0143e+00,  2.8112e-01,  ...,  1.0216e+00,\n",
       "           -3.3692e-01, -1.0323e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.1775e-02,  1.4425e-01,  1.3043e-02,  ...,  1.1622e-02,\n",
       "           -5.0581e-04, -2.7503e-02],\n",
       "          [ 5.2062e-02,  2.8698e-02,  2.3076e-02,  ..., -1.4392e-02,\n",
       "           -2.0496e-03, -2.3763e-02],\n",
       "          [ 6.1728e-02, -5.8556e-02,  7.5228e-03,  ..., -1.2471e-02,\n",
       "           -2.8484e-02, -1.7695e-02],\n",
       "          ...,\n",
       "          [ 6.3636e-02,  1.2431e-02,  8.2675e-05,  ..., -1.1277e-02,\n",
       "           -4.6422e-03, -3.2207e-02],\n",
       "          [ 6.5628e-02,  4.5629e-02,  1.1708e-02,  ...,  2.9272e-03,\n",
       "           -1.2857e-02, -2.4756e-02],\n",
       "          [ 1.1342e-01,  7.0295e-02,  2.2116e-02,  ..., -2.3899e-02,\n",
       "           -4.8458e-02, -2.5548e-02]],\n",
       "\n",
       "         [[ 5.3032e-02,  5.4029e-02,  8.4977e-03,  ...,  2.6748e-02,\n",
       "            4.3244e-02, -2.5909e-02],\n",
       "          [ 4.5645e-02,  5.3149e-02,  2.2555e-02,  ..., -5.6919e-03,\n",
       "            8.1323e-04, -1.9525e-02],\n",
       "          [ 5.7508e-02,  6.1166e-02,  2.0628e-02,  ..., -2.4846e-02,\n",
       "            2.0054e-02, -2.5682e-02],\n",
       "          ...,\n",
       "          [ 5.8946e-02,  6.3355e-02,  2.8047e-03,  ..., -1.6271e-03,\n",
       "            6.0310e-03, -2.0710e-02],\n",
       "          [ 6.5402e-02,  2.7258e-02,  9.1843e-03,  ...,  1.1162e-02,\n",
       "            8.1078e-03, -1.6436e-02],\n",
       "          [ 6.4665e-02,  3.2565e-02,  1.4411e-02,  ...,  3.0474e-02,\n",
       "           -4.6340e-03, -2.1880e-02]],\n",
       "\n",
       "         [[ 6.6096e-02,  4.5255e-02,  1.9668e-02,  ..., -1.0444e-02,\n",
       "            2.8628e-02, -1.6207e-02],\n",
       "          [ 6.1966e-02,  9.1409e-02,  2.2397e-02,  ..., -2.1045e-02,\n",
       "            1.1144e-02, -2.0647e-02],\n",
       "          [ 6.8245e-02,  4.4195e-02,  1.6255e-02,  ..., -2.3557e-02,\n",
       "            5.4660e-03, -2.3550e-02],\n",
       "          ...,\n",
       "          [ 5.9186e-02,  5.1422e-02,  4.9827e-03,  ..., -3.0876e-02,\n",
       "            8.0889e-03, -2.7141e-02],\n",
       "          [ 6.2191e-02, -1.3934e-02,  2.1498e-02,  ..., -5.5602e-03,\n",
       "           -2.9631e-02, -2.2486e-02],\n",
       "          [ 5.7421e-02,  3.5921e-02,  1.3190e-02,  ..., -2.2086e-02,\n",
       "           -1.9587e-03, -2.5402e-02]]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posemb_grid.reshape(1, gs_old, gs_old, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(250001, 768)\n",
       "      (position_embedding): Embedding(64, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      (position_embedding): Embedding(192, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): PytorchGELUTanh()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"nữ tóc nâu , áo màu đen có dòng chữ trắng trước ngực , quần_đùi màu đen , đi giày thể_thao trắng , tất cao cổ màu trắng .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pretrained_model_name_or_path': '/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(config\u001b[38;5;241m.\u001b[39mtokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(config.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state = (\n",
    "    torch.jit.load(\n",
    "        \"/home/phongtnh/Person-Search/person_rlf/clip_checkpoints/ViT-B-16.pt\",\n",
    "        map_location=\"cpu\",\n",
    "    )\n",
    "    .eval()\n",
    "    .state_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49416, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resize_token_embedding(state[\"token_embedding.weight\"], 49409).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model distribtuion is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "new_embedding = nn.Embedding(49409, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embedding.weight = nn.Parameter(state[\"token_embedding.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49408, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights = F.interpolate(\n",
    "    state[\"token_embedding.weight\"],\n",
    "    size=(49409, 768),\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "state_dict = load_file(\n",
    "    \"/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints/model.safetensors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_bias', 'logit_scale', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_model.head.bias', 'text_model.head.weight', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.head.attention.in_proj_bias', 'vision_model.head.attention.in_proj_weight', 'vision_model.head.attention.out_proj.bias', 'vision_model.head.attention.out_proj.weight', 'vision_model.head.layernorm.bias', 'vision_model.head.layernorm.weight', 'vision_model.head.mlp.fc1.bias', 'vision_model.head.mlp.fc1.weight', 'vision_model.head.mlp.fc2.bias', 'vision_model.head.mlp.fc2.weight', 'vision_model.head.probe', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
