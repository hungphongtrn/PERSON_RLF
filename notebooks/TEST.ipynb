{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phongtnh/Person-Search/person_rlf\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/phongtnh/Person-Search/vn3k_person_search/VN3K/imgs/TEST_DATA/Person1.2/output_2.jpg\"\n",
    "vi_captions = \"người đàn_ông tóc cắt ngắn , mặc áo_phông màu xám nhạt có chữ màu đỏ trên ngực áo , quần màu xám nhạt , và đi_đôi giày thể_thao màu đen với dây giày màu đỏ .\"\n",
    "en_captions = \"The man had short hair, wore a light gray T-shirt with red letters on the chest, light gray pants, and black sneakers with red shoelaces.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.siglip.modeling_siglip import SiglipVisionModel\n",
    "\n",
    "model = SiglipVisionModel.from_pretrained('/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipVisionModel(\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      (position_embedding): Embedding(256, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): PytorchGELUTanh()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from model.siglip.image_processing_siglip import SiglipImageProcessor\n",
    "\n",
    "processor = SiglipImageProcessor.from_pretrained('/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints')\n",
    "inputs = processor(Image.open(file_path), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.3020, -0.2706, -0.2941,  ..., -0.7176, -0.7333, -0.7412],\n",
       "          [-0.3176, -0.3333, -0.3725,  ..., -0.7255, -0.7333, -0.7412],\n",
       "          [-0.1451, -0.1843, -0.2235,  ..., -0.7333, -0.7333, -0.7412],\n",
       "          ...,\n",
       "          [-0.1451, -0.1608, -0.2314,  ...,  0.2078,  0.3961, -0.2863],\n",
       "          [-0.1529, -0.1451, -0.2314,  ...,  0.1059,  0.3020, -0.3098],\n",
       "          [-0.1608, -0.1373, -0.2235,  ...,  0.0431,  0.1922, -0.3176]],\n",
       "\n",
       "         [[-0.6314, -0.5922, -0.6157,  ..., -0.8196, -0.8196, -0.8275],\n",
       "          [-0.6392, -0.6549, -0.6863,  ..., -0.8196, -0.8196, -0.8275],\n",
       "          [-0.4510, -0.4980, -0.5294,  ..., -0.8196, -0.8196, -0.8275],\n",
       "          ...,\n",
       "          [-0.0510, -0.0667, -0.1451,  ...,  0.2549,  0.4431, -0.2392],\n",
       "          [-0.0588, -0.0510, -0.1451,  ...,  0.1529,  0.3490, -0.2627],\n",
       "          [-0.0667, -0.0431, -0.1373,  ...,  0.0902,  0.2392, -0.2706]],\n",
       "\n",
       "         [[-0.6471, -0.6078, -0.6392,  ..., -0.8667, -0.8510, -0.8588],\n",
       "          [-0.6549, -0.6706, -0.7176,  ..., -0.8667, -0.8510, -0.8588],\n",
       "          [-0.4745, -0.5137, -0.5686,  ..., -0.8667, -0.8510, -0.8588],\n",
       "          ...,\n",
       "          [-0.0667, -0.0824, -0.1765,  ...,  0.2392,  0.4275, -0.2549],\n",
       "          [-0.0745, -0.0667, -0.1765,  ...,  0.1373,  0.3333, -0.2784],\n",
       "          [-0.0824, -0.0588, -0.1608,  ...,  0.0745,  0.2235, -0.2863]]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 384, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-2.1495, -0.4388, -1.6633,  ...,  2.0281, -0.8127,  0.6085],\n",
       "         [-5.0171, -2.7226, -2.5749,  ..., -0.2247, -1.7627,  1.9544],\n",
       "         [-0.1994, -2.6476,  0.1012,  ..., -2.8361,  1.2211,  3.4346],\n",
       "         ...,\n",
       "         [-0.3645,  0.0398, -1.1744,  ...,  0.0483, -0.2507, -0.0863],\n",
       "         [ 2.3085, -0.7375,  1.2040,  ...,  0.2259, -1.8292, -2.2350],\n",
       "         [ 1.1640, -0.1936,  1.8980,  ..., -0.6886, -0.3480, -2.0360]]]), pooler_output=tensor([[ 4.3088e-01,  9.3276e-02,  5.2031e-01, -2.9667e-01, -2.3218e-01,\n",
       "          5.0049e-01,  1.3070e-01,  1.3383e-01, -1.5079e-01, -2.1903e-03,\n",
       "          7.3598e-03, -1.9927e-01, -3.1792e-01,  7.3743e-01, -4.9895e-01,\n",
       "          3.9500e-02, -1.4580e-01, -1.8776e-02, -1.2477e-01,  3.4925e-01,\n",
       "         -1.8927e-01, -2.0986e-01,  7.6412e-02,  3.0814e-02, -2.0037e-01,\n",
       "         -3.8367e-01, -5.2844e-01,  1.4947e-01,  8.4984e-02,  4.2733e-02,\n",
       "         -2.7477e-01, -3.4590e-01, -2.9267e-01,  5.4652e-02,  6.3239e-01,\n",
       "         -2.7130e-01,  2.3582e-02, -3.9510e-01,  2.7298e-01,  9.9828e-02,\n",
       "         -1.9664e-01,  3.1539e-01, -1.6726e+00, -7.1961e-01, -3.7481e-01,\n",
       "         -3.1770e-01, -4.5391e-02, -3.4537e-01, -9.0664e-02, -8.2448e-02,\n",
       "         -2.2704e-01, -4.2938e-01, -3.9697e-02,  3.4407e-01, -5.7752e+00,\n",
       "         -3.1952e-01,  1.3698e-01, -7.3613e-01, -9.4794e-02,  3.9802e-01,\n",
       "         -1.9876e-01,  3.8460e-01, -3.3128e-01, -2.5788e-01, -3.5298e-01,\n",
       "          2.1722e-01,  2.7389e-01,  7.1247e-01, -3.4573e-01, -8.6650e-01,\n",
       "          8.0417e-01,  3.1453e-01, -2.9359e-02,  1.5730e-01, -5.0685e-01,\n",
       "          9.2977e-02,  1.1062e-01,  1.0927e+00,  1.5932e-01,  2.4973e-01,\n",
       "          8.3738e-02, -5.4986e-02, -2.2168e-02,  5.0439e-01, -1.6165e+00,\n",
       "         -4.3689e-02, -5.7527e-01,  2.0249e-01,  2.5663e-01, -1.3744e+00,\n",
       "         -9.6065e-02, -6.9352e-02, -6.3188e-02,  7.1925e-01, -2.3678e-02,\n",
       "         -2.2683e-01, -3.2888e-01,  2.3615e-01,  2.8213e-01, -1.1465e-01,\n",
       "          1.1988e+00,  2.2207e-01, -1.6423e-01, -6.5558e-01,  1.9705e-01,\n",
       "         -5.1012e-02,  3.2702e-01, -2.4722e-02,  9.9210e-02, -9.5914e-02,\n",
       "          5.0665e-01,  1.4074e-01, -3.4359e-01, -3.7400e-01, -3.5561e-01,\n",
       "          4.8907e-01, -1.4931e+00, -2.0388e-01, -1.1294e-01, -6.8826e-01,\n",
       "         -3.4197e-01, -1.4805e-01, -5.5582e-02, -1.2160e-01, -3.4715e-02,\n",
       "         -7.1483e-01,  5.3552e-02,  4.4532e-01,  1.1163e+00, -5.3645e-01,\n",
       "         -2.0302e-01,  1.9254e+00,  7.7551e-01, -9.2990e-02, -1.8029e-01,\n",
       "         -3.6086e-01, -2.0136e-01,  8.0449e-02, -5.1791e-01, -1.4228e-01,\n",
       "         -5.2158e-02, -1.2126e-01, -5.2429e-01,  1.8642e-01,  1.2882e-01,\n",
       "         -5.3990e-02,  4.5345e-01, -3.8747e-01, -1.9155e-01, -4.4053e-01,\n",
       "          1.0959e+00, -2.5325e-01,  1.2471e+00, -2.1073e-01,  3.7999e-01,\n",
       "          1.6514e-01, -8.2883e-03,  3.9110e-01, -5.1740e-01, -1.2204e-01,\n",
       "         -2.5050e-01,  3.9987e-01, -8.3310e-03, -3.7602e-01, -5.4416e-02,\n",
       "          4.5816e-01,  6.2717e-01,  3.1321e-01,  4.6218e-01, -5.2717e-02,\n",
       "          4.9936e-01,  4.5436e-01, -8.4148e-02,  3.7382e-01,  4.0235e-01,\n",
       "          1.9230e-01,  5.7973e-01, -7.2302e-02,  3.0913e-01,  2.8947e-01,\n",
       "         -6.7507e-02,  3.9747e-01, -8.5264e-02, -3.0857e-01,  2.1458e-01,\n",
       "         -1.3508e-01,  9.9233e-01,  5.1054e-02,  8.2663e-02, -6.0570e-01,\n",
       "         -8.8655e-02,  1.7642e-01,  3.5527e-02,  3.5139e-01, -3.2044e-01,\n",
       "         -1.5996e-01,  4.7954e-01,  7.8307e-01,  6.0332e-02, -1.2131e-01,\n",
       "         -6.7861e-01, -5.6293e-02, -5.7827e-01,  2.0756e-01,  2.3499e-02,\n",
       "          2.9950e-01,  1.6838e-01,  3.0510e-01,  7.7435e-02,  4.8329e-02,\n",
       "         -4.5640e-01,  3.8421e-01, -1.3790e-01,  1.4630e-01,  4.1800e-01,\n",
       "         -1.0648e+00,  5.2675e-03, -2.6095e-01,  1.8403e-01,  3.5362e-02,\n",
       "          8.3684e-01,  3.5952e-01,  1.1995e-01,  3.5584e-01,  6.1741e-01,\n",
       "          2.4304e-01, -1.2865e-01,  1.9627e-02, -1.0129e-01, -4.9573e-02,\n",
       "         -2.0223e-02,  3.6049e-02,  4.7958e-01, -5.4678e-01, -3.9204e-01,\n",
       "          7.7358e-01,  6.3538e-02, -5.3294e-01, -1.9851e-01,  1.1535e-01,\n",
       "          2.8442e-01, -1.5992e-01,  4.3985e-01,  3.6583e-01, -3.2780e-01,\n",
       "          2.0780e-01,  3.3892e-01,  2.4004e-01, -3.6530e-01, -6.7892e-01,\n",
       "          5.1784e-01,  4.2489e-01, -4.0024e-01,  3.1543e-01, -1.3498e-02,\n",
       "          8.2397e-01, -9.9536e-02, -1.2646e+00,  2.3092e-01,  8.8717e-02,\n",
       "         -2.1060e-01,  3.7267e-01,  4.1106e-03,  1.6612e-01, -5.8068e-01,\n",
       "         -1.1168e-01,  1.0532e+00, -1.2749e+00, -6.6151e-02, -9.5286e-02,\n",
       "          4.7791e-01, -7.1328e-01,  2.7965e-01,  2.0772e-01, -2.0369e-01,\n",
       "         -6.3713e-02,  9.0583e-01,  4.6469e-01,  6.4083e-01,  3.0231e-01,\n",
       "          2.7441e-01,  4.3536e-02, -5.2495e-01,  4.8305e-01, -5.8266e+00,\n",
       "          1.0529e-02, -1.7639e-02,  2.1365e-01,  1.9274e-01, -6.4122e-01,\n",
       "          5.7700e-02, -4.8513e-01, -4.4282e-01,  3.6873e-01, -3.0370e-01,\n",
       "          8.5568e-01, -2.2886e-01,  5.1531e-01,  6.4883e-02,  8.4691e-02,\n",
       "          3.2610e-02, -1.4179e-01,  3.9861e-01, -7.4042e-01,  5.7108e-01,\n",
       "         -2.5693e-01, -5.0643e-01,  7.5727e-02, -6.9320e-02, -3.0532e-01,\n",
       "         -1.8977e-01,  1.1310e-02,  2.5609e-01, -1.4542e-01, -1.4114e-02,\n",
       "          4.6362e-02,  6.5258e-02, -4.2540e-01, -3.2239e+00, -3.1360e-02,\n",
       "         -4.9131e-01,  8.9187e-02, -5.6639e-01,  7.4655e-02,  2.8968e-01,\n",
       "          1.1291e+00,  2.2288e-02, -1.3681e-01,  1.8464e-01, -1.0466e-01,\n",
       "         -2.5603e-01,  4.3333e-02, -2.6446e-02,  1.4284e-01,  4.2236e-02,\n",
       "          8.5726e-02,  2.7472e-01, -8.3583e-01, -6.1363e-01,  1.1582e-03,\n",
       "         -9.5504e-02, -7.5266e-01,  1.0125e-01, -1.6167e-01,  3.1263e-02,\n",
       "         -5.0643e-02, -2.3007e-01,  4.6911e-02, -4.8716e-01,  1.2688e-01,\n",
       "          1.2591e-01, -2.8242e-01, -2.1918e+00, -5.0718e-02,  1.8795e-01,\n",
       "          2.7451e-01, -1.9305e-01,  2.9357e-02, -4.7367e-01,  7.0802e-01,\n",
       "         -1.2374e-01,  3.2156e-01,  4.4915e-01,  1.1742e-01, -7.7334e-02,\n",
       "          5.5968e-02, -3.2696e-01, -1.6914e-01,  2.1381e-01, -6.5917e-02,\n",
       "         -7.5728e-02, -4.2590e-01, -2.7596e-01, -3.9869e-02, -2.3781e-01,\n",
       "          2.7546e+00, -3.7678e-01, -1.0232e+00,  6.2698e-01,  1.4773e-01,\n",
       "          3.4842e-01,  6.2638e-02, -2.7535e-02, -4.3436e-02, -1.7074e-02,\n",
       "          2.6124e-01,  2.5897e-01,  4.6865e-01, -1.5272e+00,  1.9937e-01,\n",
       "          4.1077e-01,  1.5507e-01, -3.4306e-01,  3.0390e-01,  1.0937e-01,\n",
       "         -1.5390e+00, -1.3920e+00, -4.2429e-01, -1.4921e-01,  3.3593e-01,\n",
       "          1.1625e-01,  1.4435e-01,  1.9184e-01,  3.2117e-02,  1.4087e-01,\n",
       "          2.7064e-01, -3.4706e-01, -1.2476e+00, -5.1426e-01, -6.2196e-03,\n",
       "          3.0225e-01,  3.9915e-01,  7.0192e-02, -2.4276e-01,  9.1090e-01,\n",
       "          2.1046e-01, -1.7917e-01, -2.6993e-01,  1.4292e-02,  3.0052e-01,\n",
       "         -2.1515e-01,  7.0908e-02, -1.0331e+00, -2.3818e-01, -6.5406e-01,\n",
       "          3.1607e-01, -5.2179e-01,  2.8207e-01, -3.3139e-02,  1.4155e-01,\n",
       "         -2.9369e-01, -1.1015e-01,  2.9993e-01, -3.7779e-02, -2.4286e-01,\n",
       "          2.2049e-01,  6.3739e-02,  2.6522e-01,  1.7383e-01,  4.2454e-01,\n",
       "         -9.2566e-02, -1.5822e-01, -2.7911e-01,  6.8584e-02,  8.9383e-02,\n",
       "          1.9274e-01,  2.9362e-01,  7.2073e-02, -3.2005e-02, -4.4966e-01,\n",
       "         -8.7889e-02, -4.3576e-01, -1.4767e-01,  5.8641e-01,  1.9433e-01,\n",
       "         -8.8680e-02,  1.4270e-01,  2.8975e-01,  2.6098e-01,  8.3472e-02,\n",
       "          3.2116e-01,  8.0463e-02,  3.1330e-01, -1.0602e-01,  7.2039e-02,\n",
       "         -7.7282e-01, -6.8769e-01, -3.1157e-01,  9.6471e-02,  7.2831e-03,\n",
       "         -3.5731e-01,  4.0702e-01, -1.4050e-01, -1.3765e+00,  4.8300e-01,\n",
       "         -1.4600e-01,  8.0527e-03, -3.8706e-01,  1.0226e+00,  6.5517e-02,\n",
       "          5.7891e-02, -1.2710e-01,  2.1449e-01,  2.1010e-01,  8.9399e-01,\n",
       "         -7.5247e-02,  4.6114e-01,  3.0997e-02, -1.9258e-01,  2.1656e-01,\n",
       "         -2.8093e-01,  6.6210e-01, -7.3517e-02, -3.6553e-01, -3.2242e-01,\n",
       "          2.8285e-01, -3.1888e-01,  6.9526e-02,  6.7287e-01,  1.0847e+00,\n",
       "         -6.7689e-01,  5.0585e-02,  6.8503e-02, -8.8471e-01,  2.1914e-01,\n",
       "          2.9453e-01,  1.5894e-01,  1.2278e-01,  1.1149e-01,  5.8010e-01,\n",
       "          9.4558e-02,  2.5978e-01,  1.8322e-01,  6.3577e-01,  2.3552e-03,\n",
       "          5.3110e-02,  4.4116e-01,  6.5142e-02,  5.3786e-01,  4.2818e-01,\n",
       "         -7.4495e-01,  2.2992e-01,  2.5925e-01,  2.0002e-01,  5.1506e-01,\n",
       "         -2.2628e-01, -1.0456e+00, -9.4960e-03,  2.1782e-01,  1.1102e-01,\n",
       "         -4.1526e-01, -6.1830e-01,  5.1804e-01, -1.2815e-01,  7.5556e-02,\n",
       "          4.1677e-02,  9.8874e-02, -1.1584e-03,  2.2336e-01, -3.4043e-01,\n",
       "          4.5086e-01, -8.1240e-01, -1.4417e-01, -8.2861e-01, -2.5330e-01,\n",
       "          2.2458e-01,  4.6841e-02, -9.4305e-01, -1.9518e-01,  1.7048e-01,\n",
       "          6.9901e-01,  8.4315e-01, -8.0615e-02, -1.6059e-02,  1.0491e+00,\n",
       "         -2.1774e-01,  1.2007e+00,  4.3050e-01, -2.1353e-01,  2.3422e-01,\n",
       "         -1.0102e-01, -3.7218e-01, -2.1413e-01,  3.3180e-01,  7.7125e-02,\n",
       "         -3.2993e-01,  4.5043e-01, -2.6781e-01, -2.0385e-01, -2.8940e-01,\n",
       "          2.2042e-01,  1.2660e+00, -8.9590e-01, -4.8305e-02,  9.9128e-02,\n",
       "         -2.4979e-01, -1.2213e-01,  2.9306e-01,  5.6860e-01, -1.3351e-01,\n",
       "          2.8448e-01, -3.1194e-01, -1.8778e-01,  5.8656e-03, -3.7101e-01,\n",
       "         -9.2769e-01,  3.4323e-01,  2.4921e-02,  1.5621e-01, -3.2798e-01,\n",
       "         -4.9223e-01,  1.8861e-01,  1.6051e-01,  1.6144e-01,  1.5985e-01,\n",
       "          4.8680e-01, -5.6133e-02,  2.7963e-01,  2.0897e-01,  1.0369e-02,\n",
       "          6.6460e-02, -3.7970e-01, -2.2695e-01, -4.3046e-01, -2.2005e-01,\n",
       "          4.1409e-01,  4.8906e-01, -6.8311e-01,  2.4027e-02,  1.0329e-01,\n",
       "         -5.1222e-01, -4.2176e-01, -8.7661e-02, -7.9998e-01, -3.1206e-01,\n",
       "         -2.8052e-02,  5.4969e-02, -2.5223e-01,  7.2053e-01, -1.1643e-01,\n",
       "         -3.1735e-01, -2.5390e-01, -5.2807e+00,  1.0695e+00,  5.5779e-01,\n",
       "         -3.8582e-01,  1.2229e-01,  1.8337e-01,  4.4894e-01,  6.4184e-02,\n",
       "          7.5221e-02, -7.3483e-02, -9.5058e-01, -4.8482e-02, -1.4531e-01,\n",
       "         -6.9811e-01, -1.5876e-01,  1.2306e-01,  5.7142e-03, -2.1593e-02,\n",
       "          6.0825e-02, -3.6314e-01,  3.4286e-01,  3.7802e-02,  1.9058e-01,\n",
       "         -1.9863e-01, -2.8526e-01, -5.4201e-01, -8.4846e-01,  4.3686e-01,\n",
       "          1.6310e-01,  1.4226e+00, -2.4475e-02, -6.6003e-01, -7.8482e-02,\n",
       "         -7.3081e-01, -2.6852e-01, -2.8842e-01,  5.5021e-01, -1.4678e-01,\n",
       "          2.5339e-01,  7.8508e-02,  4.1529e-01, -2.8564e-01,  3.0805e-01,\n",
       "         -3.7050e-01, -3.6298e-01,  1.3035e-01, -3.3043e-02, -1.4710e-01,\n",
       "          7.3694e-01, -1.5423e-01,  3.7655e-01,  4.7903e-01,  1.6424e-03,\n",
       "         -4.7094e-01,  5.2587e-01, -1.0005e+00, -6.1520e-01,  4.6368e-02,\n",
       "         -6.0410e-01,  1.7582e+00, -1.8674e-01, -2.5928e-02,  3.6995e-01,\n",
       "         -2.5681e-01, -2.2807e-01,  1.7278e-01,  2.0630e-02, -5.8742e-01,\n",
       "         -3.6062e-01, -6.4643e-02,  5.7604e-01,  6.7623e-01, -2.2135e-01,\n",
       "         -4.0787e-01,  1.6006e+00, -4.6242e-01,  8.9808e-01, -1.5624e-01,\n",
       "          4.3268e-01, -3.4715e-02, -1.5977e-01, -5.0562e-01,  5.7246e-01,\n",
       "          2.5856e-01, -5.6177e-02,  5.4074e-01, -5.6664e-01, -9.2286e-02,\n",
       "         -1.0226e-01,  4.7450e-01, -6.6462e-02,  5.2186e-01, -2.2447e-01,\n",
       "          3.5790e-01,  2.9209e-01, -6.1747e-01, -3.9159e-01, -3.1937e-01,\n",
       "         -1.5773e-01,  3.5106e-02,  9.5166e-02, -3.1421e-01,  1.1497e-01,\n",
       "          2.1420e+00, -9.5138e-02,  3.6796e-01, -2.2165e-01, -6.9754e-01,\n",
       "          2.3266e-01,  4.6052e-02,  9.7347e-02, -6.1852e-02, -3.9795e-01,\n",
       "         -6.0710e-01, -4.3033e-01, -3.4349e-02, -2.3835e-01,  5.6235e-01,\n",
       "          5.0420e-01,  5.8919e-02,  1.0833e-02, -3.6153e-01, -2.7524e-01,\n",
       "         -5.1306e-01,  3.4175e-01, -1.7334e+00,  4.4360e-01, -3.7389e-01,\n",
       "         -8.8173e-01, -1.9516e-01, -3.1651e-01, -5.9106e-01,  7.5931e-01,\n",
       "          2.1367e-01,  7.7027e-01,  7.2592e-02,  1.6019e-02, -1.1656e+00,\n",
       "          2.4483e-01,  8.9370e-02,  3.3487e-01, -3.5080e-02,  1.0340e-01,\n",
       "         -2.1713e-01, -6.5143e-01,  8.2010e-02]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, interpolate_pos_encoding=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints\", model_max_length=187)\n",
    "# input_text = tokenizer(vi_captions, return_tensors=\"pt\", truncation=True, padding=\"max_length\", return_attention_mask=True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " 2: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.added_tokens_decoder.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1672,   355,  2353,  1735, 31029,   297,   317,  2289,   270,   594,\n",
       "          6701,   326,  1335,   297,   259,   471,  2567, 78066,  1452,   273,\n",
       "           259,   329,  4428,   677,  1262,   270,   885,   562,  2342,  1452,\n",
       "           273,   355,  2071,   534,  1079,   594,  1849,   259,   471,   268,\n",
       "           819,  1313,  1452,   273,   259,   329,  4428,   677,  1262,   270,\n",
       "           259,   793,  3767, 11095,  3571,   718, 48066,   394,   924, 71949,\n",
       "          1452,   273,   355,   278,   300,   908,   331,  2625,   718, 48066,\n",
       "          1452,   273,   355,  2071,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'người đànông tóc cắt ngắn mặc áophông màu xám nhạt có chữ màu đỏ trên ngực áo quần màu xám nhạt và điđôi giày thểthao màu đen với dây giày màu đỏ</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_text.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1672,   355,  2353,  1735, 31029,   297,   317,  2289,   270,   594,\n",
       "          6701,   326,  1335,   297,   259,   471,  2567, 78066,  1452,   273,\n",
       "           259,   329,  4428,   677,  1262,   270,   885,   562,  2342,  1452,\n",
       "           273,   355,  2071,   534,  1079,   594,  1849,   259,   471,   268,\n",
       "           819,  1313,  1452,   273,   259,   329,  4428,   677,  1262,   270,\n",
       "           259,   793,  3767, 11095,  3571,   718, 48066,   394,   924, 71949,\n",
       "          1452,   273,   355,   278,   300,   908,   331,  2625,   718, 48066,\n",
       "          1452,   273,   355,  2071,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[250000,      1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<mask>\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the token with id 25000 with <mask> and flag it as special token\n",
    "# {\n",
    "#     \"<mask>\": 250000\n",
    "# }\n",
    "tokenizer.get_vocab().update({\"<mask>\": 250000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add mask token to the tokenizer\n",
    "tokenizer.add_special_tokens({'mask_token': '<mask>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the mask token id to 250000\n",
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1672, 250000,   2353, 250000,  31029,    297,    317, 250000,    270,\n",
       "             594,   6701,    326,   1335, 250000,    259,    471,   2567,  78066,\n",
       "            1452, 250000,    259,    329,   4428,    677,   1262,    270, 250000,\n",
       "          250000, 250000, 250000,    273,    355,   2071,    534,   1079,    594,\n",
       "            1849, 250000, 250000,    268,    819,   1313,   1452,    273,    259,\n",
       "             329,   4428,    677,   1262,    270,    259,    793,   3767, 250000,\n",
       "            3571, 250000, 250000,    394, 250000,  71949,   1452,    273,    355,\n",
       "             278,    300,    908,    331,   2625,    718,  48066,   1452,    273,\n",
       "             355,   2071,      1]]),\n",
       " tensor([[ -100,  -100,  -100,  1735,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  1335,   297,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,   562,  -100,  1452,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,   259,   471,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100, 11095,  -100,  -100, 48066,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100]]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator.torch_mask_tokens(input_text.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phongtnh/miniconda3/envs/person_search/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.siglip.modeling_siglip import SiglipTextModel\n",
    "\n",
    "text_model = SiglipTextModel.from_pretrained('/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     text_outputs \u001b[38;5;241m=\u001b[39m text_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_text)\n\u001b[1;32m      4\u001b[0m text_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Person-Search/person_rlf/model/siglip/modeling_siglip.py:865\u001b[0m, in \u001b[0;36mSiglipTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\n\u001b[1;32m    866\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    867\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    868\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    869\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    870\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    871\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    872\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Person-Search/person_rlf/model/siglip/modeling_siglip.py:779\u001b[0m, in \u001b[0;36mSiglipTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    776\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    777\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 779\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# note: SigLIP's text model does not use a causal mask, unlike the original CLIP model.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# expand attention_mask\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Person-Search/person_rlf/model/siglip/modeling_siglip.py:341\u001b[0m, in \u001b[0;36mSiglipTextEmbeddings.forward\u001b[0;34m(self, input_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    338\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids[:, :seq_length]\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 341\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(input_ids)\n\u001b[1;32m    343\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(position_ids)\n\u001b[1;32m    344\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/person_search/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    text_outputs = text_model(**input_text)\n",
    "text_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "mlm_probability = 0.15\n",
    "import torch\n",
    "\n",
    "def torch_mask_tokens(inputs: Any, special_tokens_mask: Optional[Any] = None):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "    \"\"\"\n",
    "    labels = {\n",
    "        \"input_ids\": inputs[\"input_ids\"].clone(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].clone(),\n",
    "    }\n",
    "\n",
    "    # We sample a few tokens in each sequence for MLM training (with probability `mlm_probability`)\n",
    "    probability_matrix = torch.full(labels['input_ids'].shape, mlm_probability)\n",
    "    if special_tokens_mask is None:\n",
    "        special_tokens_mask = [\n",
    "            tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels['input_ids'].tolist()\n",
    "        ]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "    else:\n",
    "        special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels['input_ids'][~masked_indices] = 1  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels['input_ids'].shape, 0.8)).bool() & masked_indices\n",
    "    inputs['input_ids'][indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels['input_ids'].shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels['input_ids'].shape, dtype=torch.long)\n",
    "    inputs['input_ids'][indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels\n",
    "\n",
    "inputs, labels = torch_mask_tokens(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  1672,    355,   2353,   1735,  31029,    297,    317,   2289,    270,\n",
       "            594,   6701,    326,   1335,    297,    259,    471,   2567,  78066,\n",
       "           1452,    273,    259,    329,   4428,    677,   1262,    270,    885,\n",
       "            562,   2342,   1452, 250000,    355,   2071,    534, 250000,    594,\n",
       "           1849,    259,    471,    268,    819, 250000,   1452,    273,    259,\n",
       "            329,   4428, 250000,   1262, 250000,    259,    793, 250000,  11095,\n",
       "         250000,    718,  48066,    394,    924,  71949, 250000,    273,    355,\n",
       "            278,    300,    908, 128128,   2625,    718,  48066,   1452,    273,\n",
       "            355,   2071,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,  273,    1,    1,    1, 1079,    1,\n",
       "           1,    1,    1,    1,    1, 1313,    1,    1,    1,    1,    1,  677,\n",
       "           1,  270,    1,  793, 3767,    1, 3571,    1,    1,    1,    1,    1,\n",
       "        1452,    1,    1,    1,    1,    1,  331,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['input_ids'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'người đ<mask> ông tóc cắt ng<mask><mask> ặc áophông màu <mask> ám nhạt có chữ màu<mask> ỏ trên ngực áo quần<mask> u<mask> xám<mask> ạ<mask> và đi<mask> ôi gimuč thểthao<mask> u<mask><mask> với dây giày màu đỏ</s>'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'id': 1,\n",
       " 'file_path': 'TEST_DATA/Person1.2/output_2.jpg',\n",
       " 'split': 'train',\n",
       " 'captions': ['người đàn_ông tóc cắt ngắn , mặc áo_phông màu xám nhạt có chữ màu đỏ trên ngực áo , quần màu xám nhạt , và đi_đôi giày thể_thao màu đen với dây giày màu đỏ .',\n",
       "  'người đàn_ông dáng đi vai hơi gù , mặc bộ quần_áo màu xám nhạt , sải bước chân rộng với đôi giày màu đen thắt dây màu đỏ .'],\n",
       " 'processed_tokens': [['người',\n",
       "   'đàn_ông',\n",
       "   'tóc',\n",
       "   'cắt',\n",
       "   'ngắn',\n",
       "   ',',\n",
       "   'mặc',\n",
       "   'áo_phông',\n",
       "   'màu',\n",
       "   'xám',\n",
       "   'nhạt',\n",
       "   'có',\n",
       "   'chữ',\n",
       "   'màu',\n",
       "   'đỏ',\n",
       "   'trên',\n",
       "   'ngực',\n",
       "   'áo',\n",
       "   ',',\n",
       "   'quần',\n",
       "   'màu',\n",
       "   'xám',\n",
       "   'nhạt',\n",
       "   ',',\n",
       "   'và',\n",
       "   'đi_đôi',\n",
       "   'giày',\n",
       "   'thể_thao',\n",
       "   'màu',\n",
       "   'đen',\n",
       "   'với',\n",
       "   'dây',\n",
       "   'giày',\n",
       "   'màu',\n",
       "   'đỏ',\n",
       "   '.'],\n",
       "  ['người',\n",
       "   'đàn_ông',\n",
       "   'dáng',\n",
       "   'đi',\n",
       "   'vai',\n",
       "   'hơi',\n",
       "   'gù',\n",
       "   ',',\n",
       "   'mặc',\n",
       "   'bộ',\n",
       "   'quần_áo',\n",
       "   'màu',\n",
       "   'xám',\n",
       "   'nhạt',\n",
       "   ',',\n",
       "   'sải',\n",
       "   'bước',\n",
       "   'chân',\n",
       "   'rộng',\n",
       "   'với',\n",
       "   'đôi',\n",
       "   'giày',\n",
       "   'màu',\n",
       "   'đen',\n",
       "   'thắt',\n",
       "   'dây',\n",
       "   'màu',\n",
       "   'đỏ',\n",
       "   '.']],\n",
       " 'en_captions': ['The man had short hair, wore a light gray T-shirt with red letters on the chest, light gray pants, and black sneakers with red shoelaces.',\n",
       "  'The man was hunched - shouldered, dressed in light gray clothes, with a wide stride and black shoes with red laces.']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "with open(\"/home/phongtnh/Person-Search/vn3k_person_search/VN3K/data_captions.json\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the maximum length of the captions\n",
    "max = 0\n",
    "for d in data:\n",
    "    captions = d['captions']\n",
    "    en_captions = d['en_captions']\n",
    "    # for cap in captions:\n",
    "    #     l = tokenizer(cap, return_tensors=\"pt\").input_ids.shape[1]\n",
    "    #     if l > max:\n",
    "    #         max = l\n",
    "    for cap in en_captions:\n",
    "        l = tokenizer(cap, return_tensors=\"pt\").input_ids.shape[1]\n",
    "        if l > max:\n",
    "            max = l\n",
    "\n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file='/home/phongtnh/Person-Search/person_rlf/siglip_checkpoints/spiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250100"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "person_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
